{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bd2f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUPYTER WORKFLOW (safe mode): do NOT reset DB -> if DB exists, raise error\n",
    "# -------------------------------------------------------------------------\n",
    "# Steps:\n",
    "# 1) Check DB state: if CSV or DB folder already exist -> raise an error (abort).\n",
    "# 2) Initialize Oracle -> will create fresh CSV header and DB structure.\n",
    "# 3) Generate N compositions from Config elements and run Oracle for each.\n",
    "# 4) Build DataLoaders, create model, and train on the freshly created data.\n",
    "#\n",
    "# Notes:\n",
    "# - Code comments are in English (per your preference).\n",
    "# - No argparse; tweak parameters at the top of this cell.\n",
    "\n",
    "# =========================\n",
    "# Parameters (edit here)\n",
    "# =========================\n",
    "N_POINTS = 5000            # number of data points (compositions) to generate\n",
    "SEED = 42               # random seed for composition sampling\n",
    "VAL_SPLIT = None        # set to a float (e.g., 0.2) to override Config.val_split, or leave None\n",
    "EPOCHS = None           # set to an int to override Config.epochs, or leave None\n",
    "DISABLE_WANDB = False    # True: disable wandb for quick tests\n",
    "\n",
    "# =========================\n",
    "# Imports and path setup\n",
    "# =========================\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Ensure module path (adjust if needed)\n",
    "MODULE_DIRS = [\"/mnt/data\", \".\"]\n",
    "for d in MODULE_DIRS:\n",
    "    if d not in sys.path:\n",
    "        sys.path.insert(0, d)\n",
    "\n",
    "# Project modules expected to be available\n",
    "from config import Config\n",
    "from oracle import Oracle\n",
    "from dataset import create_dataloaders\n",
    "from template_graph_builder import TemplateGraphBuilder\n",
    "from model import create_model_from_config\n",
    "from trainer import Trainer\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Helper functions\n",
    "# =========================\n",
    "def print_header(title: str):\n",
    "    \"\"\"Pretty section header for console output.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(title)\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "def ensure_database_absent_or_raise(cfg: Config) -> None:\n",
    "    \"\"\"\n",
    "    Enforce a clean start WITHOUT deleting anything.\n",
    "    If CSV or DB directory already exist, raise an error and abort.\n",
    "    \"\"\"\n",
    "    csv_path = Path(cfg.csv_path)\n",
    "    db_dir = Path(cfg.database_dir)\n",
    "\n",
    "    exists_csv = csv_path.exists()\n",
    "    exists_db_dir = db_dir.exists() and any(db_dir.iterdir())  # treat empty dir as \"exists with content\"\n",
    "\n",
    "    if exists_csv or exists_db_dir:\n",
    "        # Build a helpful error message\n",
    "        msg_lines = [\"Existing database detected – refusing to overwrite.\",\n",
    "                     f\"- CSV path: {csv_path}  -> {'EXISTS' if exists_csv else 'missing'}\",\n",
    "                     f\"- DB dir:   {db_dir}   -> {'EXISTS & non-empty' if exists_db_dir else ('exists & empty' if db_dir.exists() else 'missing')}\",\n",
    "                     \"\",\n",
    "                     \"To proceed, move or delete the existing database/CSV and re-run this cell.\"]\n",
    "        raise FileExistsError(\"\\n\".join(msg_lines))\n",
    "\n",
    "def dirichlet_compositions(elements, n_points, seed=42):\n",
    "    \"\"\"\n",
    "    Generate n compositions on the simplex using a Dirichlet distribution.\n",
    "    Returns a list of dicts: {element: fraction}, fractions sum to 1.0.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    alpha = np.ones(len(elements), dtype=float)  # uniform Dirichlet\n",
    "    samples = rng.dirichlet(alpha, size=n_points)\n",
    "\n",
    "    comps = []\n",
    "    for row in samples:\n",
    "        row = np.clip(row, 0.0, 1.0)\n",
    "        row = row / row.sum()\n",
    "        comps.append({el: float(fr) for el, fr in zip(elements, row)})\n",
    "\n",
    "    # Ensure the equimolar point appears (helpful for sanity)\n",
    "    if n_points >= 1:\n",
    "        equi = {el: 1.0 / len(elements) for el in elements}\n",
    "        comps[0] = equi\n",
    "\n",
    "    return comps\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main notebook workflow\n",
    "# =========================\n",
    "# 1) Load and optionally tweak config\n",
    "print_header(\"CONFIG SUMMARY\")\n",
    "config = Config()\n",
    "\n",
    "if DISABLE_WANDB:\n",
    "    config.use_wandb = False  # turn off wandb logging for quick tests\n",
    "\n",
    "if EPOCHS is not None:\n",
    "    config.epochs = int(EPOCHS)\n",
    "\n",
    "if VAL_SPLIT is not None:\n",
    "    config.val_split = float(VAL_SPLIT)\n",
    "\n",
    "print(f\"Elements (from Config): {config.elements}\")\n",
    "print(f\"Database dir:          {config.database_dir}\")\n",
    "print(f\"CSV path:              {config.csv_path}\")\n",
    "print(f\"W&B enabled:           {config.use_wandb}\")\n",
    "print(f\"Epochs:                {config.epochs}\")\n",
    "print(f\"Val split:             {config.val_split}\")\n",
    "\n",
    "# 2) Safety check: do NOT reset DB; if something exists, raise error\n",
    "print_header(\"SAFETY CHECK (DB must be absent)\")\n",
    "ensure_database_absent_or_raise(config)\n",
    "print(\"✓ No existing DB/CSV detected -> safe to create a new one.\")\n",
    "\n",
    "# 3) Initialize Oracle (this will create the CSV header and required folders)\n",
    "print_header(\"INIT ORACLE\")\n",
    "oracle = Oracle(config)  # expected to create fresh CSV header and structure\n",
    "\n",
    "# 4) Build composition list of length N_POINTS based on elements in Config\n",
    "print_header(\"GENERATE COMPOSITIONS\")\n",
    "elements = list(config.elements)\n",
    "comps = dirichlet_compositions(elements, n_points=N_POINTS, seed=SEED)\n",
    "for i, c in enumerate(comps, 1):\n",
    "    fr_str = \", \".join(f\"{k}={v:.3f}\" for k, v in c.items())\n",
    "    print(f\"[{i:02d}] {fr_str}\")\n",
    "\n",
    "# 5) Run NEB/CHGNet for each composition to populate the database\n",
    "print_header(\"POPULATE DATABASE (NEB runs)\")\n",
    "t0 = time.time()\n",
    "successes = 0\n",
    "for idx, comp in enumerate(comps, 1):\n",
    "    ok = oracle.calculate(comp)  # performs full pipeline and writes CSV/structures\n",
    "    if ok:\n",
    "        successes += 1\n",
    "dt = time.time() - t0\n",
    "print(f\"\\n✓ Finished generating data: {successes}/{len(comps)} successful in {dt/60.0:.1f} min\")\n",
    "\n",
    "if successes == 0:\n",
    "    raise RuntimeError(\"No successful data points were created; aborting training.\")\n",
    "\n",
    "# 6) Create dataloaders (reads config.csv_path, filters barriers, builds graphs on-the-fly)\n",
    "print_header(\"CREATE DATALOADERS\")\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    config,\n",
    "    val_split=config.val_split,\n",
    "    random_seed=config.random_seed\n",
    ")\n",
    "\n",
    "# 7) Determine dynamic node_input_dim using the template graph builder\n",
    "#    Node features are: 3 (positions) + N (one-hot) + 4 (atomic properties)\n",
    "print_header(\"BUILD TEMPLATE & MODEL\")\n",
    "builder = TemplateGraphBuilder(config, csv_path=config.csv_path)\n",
    "node_input_dim = 3 + len(builder.elements) + 4\n",
    "print(f\"Detected elements in DB: {builder.elements}\")\n",
    "print(f\"node_input_dim = 3 (pos) + {len(builder.elements)} (one-hot) + 4 (props) = {node_input_dim}\")\n",
    "\n",
    "# 8) Create model and trainer\n",
    "model = create_model_from_config(config, node_input_dim=node_input_dim)\n",
    "trainer = Trainer(model, config, save_dir=config.checkpoint_dir)\n",
    "\n",
    "# 9) Train\n",
    "print_header(\"TRAINING\")\n",
    "history = trainer.train(train_loader, val_loader, verbose=True)\n",
    "\n",
    "print_header(\"DONE\")\n",
    "print(\"Best validation loss:\", trainer.best_val_loss)\n",
    "print(\"Checkpoints:\", Path(config.checkpoint_dir).resolve())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wandb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
