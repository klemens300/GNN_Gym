{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1081d867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Active Learning Loop with Logging and Cycle Management ===\n",
    "# Console: German; Comments: English\n",
    "# Run this cell directly in VSCode / Jupyter.\n",
    "\n",
    "import sys\n",
    "import traceback\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# --- Project imports ---\n",
    "from config import Config\n",
    "from oracle import Oracle\n",
    "from inference import run_inference_cycle, cleanup_gpu\n",
    "from trainer import Trainer\n",
    "from dataset import create_dataloaders\n",
    "from template_graph_builder import TemplateGraphBuilder\n",
    "from model import create_model_from_config, count_parameters\n",
    "from utils import get_node_input_dim, save_model_for_inference\n",
    "\n",
    "# --- Optional Weights & Biases support ---\n",
    "try:\n",
    "    import wandb\n",
    "    WANDB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    WANDB_AVAILABLE = False\n",
    "\n",
    "# ============================================================================\n",
    "# Setup Main Logger\n",
    "# ============================================================================\n",
    "\n",
    "def setup_main_logger(config: Config):\n",
    "    \"\"\"Setup main logger for active learning loop.\"\"\"\n",
    "    logger = logging.getLogger(\"active_learning\")\n",
    "    logger.setLevel(getattr(logging, config.log_level.upper()))\n",
    "    logger.handlers = []  # Clear existing handlers\n",
    "    \n",
    "    # File handler\n",
    "    log_file = Path(config.log_dir) / \"active_learning.log\"\n",
    "    log_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    fh = logging.FileHandler(log_file)\n",
    "    fh.setLevel(getattr(logging, config.log_level.upper()))\n",
    "    \n",
    "    # Console handler\n",
    "    if config.log_to_console:\n",
    "        ch = logging.StreamHandler()\n",
    "        ch.setLevel(getattr(logging, config.log_level.upper()))\n",
    "    \n",
    "    # Formatter\n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s | %(levelname)-8s | %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "    fh.setFormatter(formatter)\n",
    "    if config.log_to_console:\n",
    "        ch.setFormatter(formatter)\n",
    "    \n",
    "    # Add handlers\n",
    "    logger.addHandler(fh)\n",
    "    if config.log_to_console:\n",
    "        logger.addHandler(ch)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "# ============================================================================\n",
    "# Helper functions\n",
    "# ============================================================================\n",
    "\n",
    "def is_csv_missing_or_empty(csv_path: str) -> bool:\n",
    "    \"\"\"Check if the CSV file is missing or empty.\"\"\"\n",
    "    p = Path(csv_path)\n",
    "    if not p.exists():\n",
    "        return True\n",
    "    try:\n",
    "        df = pd.read_csv(p)\n",
    "        return len(df) == 0\n",
    "    except Exception:\n",
    "        return True\n",
    "\n",
    "\n",
    "def sample_simplex_uniform(n: int, k: int) -> np.ndarray:\n",
    "    \"\"\"Sample n points uniformly on a (k-1)-simplex using Dirichlet distribution.\"\"\"\n",
    "    return np.random.dirichlet(alpha=np.ones(k), size=n)\n",
    "\n",
    "\n",
    "def initial_data_creation_if_needed(config: Config, oracle: Oracle, logger: logging.Logger):\n",
    "    \"\"\"\n",
    "    Create an initial dataset if CSV is missing or empty.\n",
    "    Uses config.al_initial_samples to determine the number of samples.\n",
    "    \"\"\"\n",
    "    csv_path = config.csv_path\n",
    "    if not is_csv_missing_or_empty(csv_path):\n",
    "        logger.info(f\"✓ Database found: {csv_path}\")\n",
    "        return\n",
    "\n",
    "    elements = list(getattr(config, \"elements\", []))\n",
    "    n_seed = int(getattr(config, \"al_initial_samples\", 0) or 0)\n",
    "    if not elements or n_seed <= 0:\n",
    "        raise RuntimeError(\n",
    "            \"Initial data creation requires 'elements' and 'al_initial_samples' > 0 in config.\"\n",
    "        )\n",
    "\n",
    "    logger.info(\"=\"*70)\n",
    "    logger.info(\"INITIAL DATA CREATION (CSV empty or not found)\")\n",
    "    logger.info(\"=\"*70)\n",
    "    logger.info(f\"Elements: {elements}\")\n",
    "    logger.info(f\"Samples to create: {n_seed}\")\n",
    "    logger.info(f\"Target CSV: {csv_path}\")\n",
    "\n",
    "    weights = sample_simplex_uniform(n_seed, len(elements))\n",
    "    compositions = []\n",
    "    for row in weights:\n",
    "        comp = {el: float(val) for el, val in zip(elements, row)}\n",
    "        s = sum(comp.values())\n",
    "        if abs(s - 1.0) > 1e-12:\n",
    "            for el in comp:\n",
    "                comp[el] /= s\n",
    "        compositions.append(comp)\n",
    "\n",
    "    successes = 0\n",
    "    for i, comp in enumerate(compositions, 1):\n",
    "        try:\n",
    "            logger.info(f\"  [{i}/{n_seed}] Calculating: {comp}\")\n",
    "            ok = oracle.calculate(comp)\n",
    "            if ok is not False:\n",
    "                successes += 1\n",
    "        except Exception as e:\n",
    "            logger.error(f\"   ✗ Error at sample {i}: {e}\")\n",
    "\n",
    "    if successes == 0:\n",
    "        raise RuntimeError(\"Initial data creation failed: no valid samples added.\")\n",
    "    logger.info(f\"✓ Initial data creation completed. {successes} samples added.\")\n",
    "\n",
    "\n",
    "def get_database_stats(csv_path: str) -> dict:\n",
    "    \"\"\"Return summary statistics for the current database CSV.\"\"\"\n",
    "    p = Path(csv_path)\n",
    "    if not p.exists():\n",
    "        return {\"n_samples\": 0, \"n_compositions\": 0}\n",
    "    try:\n",
    "        df = pd.read_csv(p)\n",
    "    except Exception:\n",
    "        return {\"n_samples\": 0, \"n_compositions\": 0}\n",
    "    return {\n",
    "        \"n_samples\": len(df),\n",
    "        \"n_compositions\": df['composition_string'].nunique() if 'composition_string' in df.columns else 0\n",
    "    }\n",
    "\n",
    "\n",
    "def train_cycle_model(config: Config, cycle: int, logger: logging.Logger) -> dict:\n",
    "    \"\"\"Train the model for the given active learning cycle.\"\"\"\n",
    "    logger.info(\"=\"*70)\n",
    "    logger.info(f\"TRAINING MODEL - CYCLE {cycle}\")\n",
    "    logger.info(\"=\"*70)\n",
    "\n",
    "    outdir = Path(config.checkpoint_dir) / f\"cycle_{cycle}\"\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        # Get current database size\n",
    "        db_stats = get_database_stats(config.csv_path)\n",
    "        n_samples = db_stats['n_samples']\n",
    "        \n",
    "        train_loader, val_loader = create_dataloaders(config)\n",
    "        builder = TemplateGraphBuilder(config)\n",
    "        node_input_dim = get_node_input_dim(builder)\n",
    "        model = create_model_from_config(config, node_input_dim)\n",
    "        \n",
    "        # Pass cycle and n_samples to trainer for naming\n",
    "        trainer = Trainer(model, config, save_dir=str(outdir), cycle=cycle)\n",
    "        trainer.train(train_loader, val_loader, verbose=True)\n",
    "        \n",
    "        logger.info(f\"✓ Model training completed (Cycle {cycle})\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"✗ Training failed: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "def active_learning_loop(config: Config, logger: logging.Logger):\n",
    "    \"\"\"Main active learning loop with initial data creation.\"\"\"\n",
    "    logger.info(\"=\"*70)\n",
    "    logger.info(\"ACTIVE LEARNING LOOP STARTING\")\n",
    "    logger.info(\"=\"*70)\n",
    "    logger.info(f\"Cycles: {config.al_max_cycles}\")\n",
    "    logger.info(f\"Test samples per cycle: {config.al_n_test}\")\n",
    "    logger.info(f\"Query samples per cycle: {config.al_n_query}\")\n",
    "    logger.info(f\"Elements: {config.elements}\")\n",
    "    logger.info(\"=\"*70)\n",
    "\n",
    "    oracle = Oracle(config)\n",
    "    initial_data_creation_if_needed(config, oracle, logger)\n",
    "\n",
    "    # CRITICAL FIX: Use range(config.al_max_cycles) directly, do NOT multiply\n",
    "    for cycle in range(config.al_max_cycles):\n",
    "        logger.info(\"=\"*70)\n",
    "        logger.info(f\"Cycle {cycle}/{config.al_max_cycles - 1}\")\n",
    "        logger.info(\"=\"*70)\n",
    "\n",
    "        db_stats = get_database_stats(config.csv_path)\n",
    "        logger.info(f\"Current database: {db_stats['n_samples']} samples, {db_stats['n_compositions']} compositions\")\n",
    "\n",
    "        # Model path logic\n",
    "        if cycle == 0:\n",
    "            logger.info(\"→ Training initial model ...\")\n",
    "            train_cycle_model(config, cycle, logger)\n",
    "        else:\n",
    "            prev_model = Path(config.checkpoint_dir) / f\"cycle_{cycle-1}\" / \"best_model.pt\"\n",
    "            if not prev_model.exists():\n",
    "                logger.warning(\"⚠️ Previous model not found, training new model ...\")\n",
    "                train_cycle_model(config, cycle, logger)\n",
    "            else:\n",
    "                logger.info(f\"→ Using model: {prev_model}\")\n",
    "\n",
    "        # Determine current model path\n",
    "        current_model = Path(config.checkpoint_dir) / f\"cycle_{cycle}\" / \"best_model.pt\"\n",
    "        \n",
    "        # Run inference with CURRENT cycle number (not cycle*2!)\n",
    "        logger.info(\"→ Starting inference cycle ...\")\n",
    "        try:\n",
    "            run_inference_cycle(cycle, str(current_model), oracle, config, verbose=True)\n",
    "            \n",
    "            # After inference, train next model\n",
    "            if cycle < config.al_max_cycles - 1:\n",
    "                logger.info(\"→ Training model with new data ...\")\n",
    "                train_cycle_model(config, cycle + 1, logger)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"✗ Inference failed: {e}\")\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "\n",
    "        logger.info(f\"✓ Cycle {cycle} completed.\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Execution (show config + confirmation)\n",
    "# ============================================================================\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Setup main logger\n",
    "logger = setup_main_logger(config)\n",
    "\n",
    "# Print config summary for user review\n",
    "logger.info(\"=\"*70)\n",
    "logger.info(\"CURRENT CONFIG\")\n",
    "logger.info(\"=\"*70)\n",
    "for key, val in config.__dict__.items():\n",
    "    logger.info(f\"{key:25s}: {val}\")\n",
    "logger.info(\"=\"*70)\n",
    "\n",
    "confirm = input(\"❓ Do you want to start the Active Learning workflow? (y/n): \").strip().lower()\n",
    "if confirm == \"y\":\n",
    "    try:\n",
    "        active_learning_loop(config, logger)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"✗ Run aborted: {e}\")\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    logger.info(\"↪️  Aborted – no execution started.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wandb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
